import Latex from '../components/Latex'
import CodeBlock from '../components/CodeBlock'
import DiracDeltaPlot from '../components/DiracDeltaPlot'
import EntropyBarChart from '../components/EntropyBarChart'
import CoinEntropyPlot from '../components/CoinEntropyPlot'
import KLDivHeatmap from '../components/KLDivHeatmap'

# ğŸ” Shannon Entropy: The Art of Quantifying Uncertainty

Shannon entropy is one of those rare ideas that connects philosophy, coding, machine learning, and physics.

At its core, it answers this question:

> â€œHow uncertain am I about the outcome of a random variable?â€

And it does it with elegance and precision. Letâ€™s explore.

---

## ğŸ“¦ Self-Information: Surprise in a Single Event

Before we talk entropy, letâ€™s define *self-information*:

<Latex math="I(x) = -\log P(x)" block />

This measures how *surprising* an event \( x \) is.

If \( P(x) = 1 \), the surprise is 0. You already knew it would happen.

If \( P(x) = 0.01 \), itâ€™s very surprising.

### ğŸ” Example

<CodeBlock language="python">
{`
from math import log2

def self_info(p):
    return -log2(p)

print(self_info(0.9))   # Low surprise
print(self_info(0.01))  # High surprise
`}
</CodeBlock>

This tells us: **learning that something rare happened gives us more information**.

---

## ğŸ”¢ Shannon Entropy: The Average Surprise

Entropy is just the expected value of self-information:

<Latex math="H(X) = -\sum p(x_i) \log p(x_i)" block />

It tells us how much information (in bits) weâ€™d gain *on average* by observing an outcome from \( X \).

<EntropyBarChart />

### âœï¸ Example: Manual Entropy Calculation

Letâ€™s say we have a variable with values:

- Red: 0.5  
- Green: 0.25  
- Blue: 0.25

<CodeBlock language="python">
{`
def entropy(p):
    return -sum(x * log2(x) for x in p)

print(entropy([0.5, 0.25, 0.25]))  # â‰ˆ 1.5 bits
`}
</CodeBlock>

This means it takes ~1.5 bits on average to encode a sample from this distribution.

---

## ğŸª™ Entropy of a Biased Coin

Entropy reaches its max when the coin is fair (50/50), and drops as it gets biased.

<CoinEntropyPlot />

### ğŸ§  In Code:

<CodeBlock language="python">
{`
import numpy as np
import matplotlib.pyplot as plt

p = np.linspace(0.01, 0.99, 100)
H = -p * np.log2(p) - (1 - p) * np.log2(1 - p)

plt.plot(p, H)
plt.title('Entropy of a Biased Coin')
plt.xlabel('P(Heads)')
plt.ylabel('Entropy (bits)')
plt.grid(True)
plt.show()
`}
</CodeBlock>

---

## ğŸ” Cross Entropy and KL Divergence

When we compare two distributions, we use **cross-entropy** and **KL divergence**.

### Cross Entropy:

<Latex math="H(P, Q) = -\sum_x P(x) \log Q(x)" block />

It measures how costly it is to encode samples from \( P \) using a code optimized for \( Q \).

### KL Divergence:

<Latex math="D_{KL}(P || Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}" block />

Itâ€™s not symmetric, but itâ€™s very powerful.

<KLDivHeatmap />

### ğŸ” Example

<CodeBlock language="python">
{`
def kl_div(p, q):
    return sum(p_i * log2(p_i / q_i) for p_i, q_i in zip(p, q))

print(kl_div([0.8, 0.2], [0.6, 0.4]))  # â‰ˆ 0.09 bits
`}
</CodeBlock>

---

## ğŸ§  Entropy in ML: Decision Trees

In classification, entropy tells us how â€œmixedâ€ the classes are in a group.

<CodeBlock language="python">
{`
# 9 positives, 5 negatives
pos, neg = 9, 5
total = pos + neg

p_pos = pos / total
p_neg = neg / total

print(entropy([p_pos, p_neg]))  # â‰ˆ 0.94 bits
`}
</CodeBlock>

A feature that reduces entropy gives us **information gain** â€” thatâ€™s how trees decide where to split!

---

## ğŸŒ Mutual Information: When Two Things Are Linked

Mutual information measures how much knowing one variable tells us about another.

<Latex math="I(X; Y) = H(X) + H(Y) - H(X, Y)" block />

If \( I(X; Y) = 0 \), theyâ€™re independent.  
If itâ€™s positive, they share information.

---

## ğŸ¯ Functional Dependency and the Dirac Delta

When \( Y = f(X) \), then \( Y \) is fully determined by \( X \).

We represent this with:

<Latex math="F(x, y) = F(x)\delta(y - f(x))" block />

<DiracDeltaPlot />

In such cases, the **conditional entropy** \( H(Y|X) = 0 \), because there's no uncertainty left in \( Y \) once \( X \) is known.

---

## ğŸ§ª Real World = Noisy, Not Perfect

In practice, we never get fully deterministic variables. But we **try** to find those with low conditional entropy:

> â€œCan I predict Y well, if I know X?â€

Thatâ€™s what information theory helps us do â€” quantify how good our guess is.

---

## ğŸ”š Recap

- Entropy = average surprise  
- Self-information = surprise of a single event  
- KL divergence = difference between beliefs  
- Mutual info = shared insight between variables  
- Trees, clusters, anomalies â€” they all use entropy

> Information theory is like a flashlight.  
> It doesnâ€™t remove uncertainty, but it helps us see how deep it goes.

---
